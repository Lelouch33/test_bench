#!/usr/bin/env python3
"""
Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ nonce Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
  python3 deduplicate_nonces.py results/*.json
  python3 deduplicate_nonces.py results/*.json --output merged_results.json
"""

import json
import argparse
import sys
from pathlib import Path
from typing import Set, List, Dict, Any

# ĞšĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° gonka
WEIGHT_SCALE_FACTOR = 2.5


def load_results(files: List[Path]) -> List[Dict[str, Any]]:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ· JSON Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²"""
    results = []
    for f in files:
        try:
            with open(f, 'r') as file:
                data = json.load(file)
                data['_source_file'] = str(f)
                results.append(data)
        except Exception as e:
            print(f"âš  ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ {f}: {e}")
    return results


def merge_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹"""

    all_nonces: Set[int] = set()
    total_raw_checked = 0
    sources = []

    for r in results:
        # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ nonce - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¸Ñ…
        if 'all_valid_nonces' in r:
            all_nonces.update(r['all_valid_nonces'])
        # Ğ˜Ğ½Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¸Ğ· CSV Ñ„Ğ°Ğ¹Ğ»Ğ°
        else:
            nonce_file = Path(r['_source_file']).parent / Path(r['_source_file']).stem.replace("_nonces", "") + "_nonces.csv"
            if nonce_file.exists():
                import csv
                with open(nonce_file, 'r') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        all_nonces.add(int(row['nonce']))

        total_raw_checked += r.get('total_checked', 0)
        sources.append({
            'file': Path(r['_source_file']).name,
            'valid_nonces': r.get('valid_nonces', 0),
            'poc_weight': r.get('poc_weight', 0),
            'timestamp': r.get('timestamp', ''),
        })

    unique_count = len(all_nonces)
    duplicates = sum(r.get('valid_nonces', 0) for r in results) - unique_count

    # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ aggregated poc_weight
    unique_poc_weight = int(unique_count * WEIGHT_SCALE_FACTOR)

    merged = {
        'merged_from': len(results),
        'sources': sources,
        'unique_valid_nonces': unique_count,
        'unique_poc_weight': unique_poc_weight,
        'total_raw_checked': total_raw_checked,
        'duplicates_removed': duplicates,
    }

    return merged


def print_merged(merged: Dict[str, Any]):
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ"""

    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                    Ğ”Ğ•Ğ”Ğ£ĞŸĞ›Ğ˜ĞšĞĞ¦Ğ˜Ğ¯ NONCE                         â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")

    print(f"â•‘  ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²:    {merged['merged_from']:<38}â•‘")
    print(f"â•‘  Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… nonce:     {merged['unique_valid_nonces']:<38}â•‘")
    print(f"â•‘  Ğ”ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾:   {merged['duplicates_removed']:<38}â•‘")
    print(f"â•‘                                                           â•‘")
    print(f"â•‘  {GREEN}unique_poc_weight:     {merged['unique_poc_weight']:<38}â•‘{END}")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    print("\nğŸ“Š Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:")
    for s in merged['sources']:
        print(f"  â€¢ {s['file']}: valid={s['valid_nonces']}, weight={s['poc_weight']}")


GREEN = '\033[92m'
END = '\033[0m'


def main():
    parser = argparse.ArgumentParser(
        description="Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ nonce Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²"
    )
    parser.add_argument(
        "files",
        nargs="+",
        help="JSON Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸"
    )
    parser.add_argument(
        "--output", "-o",
        help="Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² Ñ„Ğ°Ğ¹Ğ»"
    )

    args = parser.parse_args()

    files = [Path(f) for f in args.files]
    results = load_results(files)

    if not results:
        print("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
        return 1

    merged = merge_results(results)
    print_merged(merged)

    print(f"\nğŸ“ Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: poc_weight = unique_valid_nonces Ã— {WEIGHT_SCALE_FACTOR}")

    if args.output:
        with open(args.output, 'w') as f:
            json.dump(merged, f, indent=2)
        print(f"\nâœ“ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {args.output}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
